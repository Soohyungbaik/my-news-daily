import datetime
import requests
from bs4 import BeautifulSoup
import re
import json
import os

today = datetime.date.today().strftime('%Y-%m-%d')
output_path = f"news_items_{today}.json"

# ğŸ” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì†Œë¬¸ì ë³€í™˜ ê¸°ì¤€)
keywords = [
 # í•œêµ­ì–´
    "ì„œë¸Œì»¬ì²˜", "ìˆ˜ì§‘í˜•", "ë¯¸ì†Œë…€", "ê²Œì„ì‡¼", "êµ¿ìŠ¤ë§ˆì¼", "ì½”ìŠ¤í”„ë ˆ", "ë¶€ìŠ¤", "ì½œë¼ë³´", "ëŸ°ì¹­", "ì—…ê³„ ë™í–¥", "ì‹œì¥ ë³´ê³ ì„œ",
    "ë‹ˆì¼€", "ë¸”ë£¨ì•„ì¹´ì´ë¸Œ", "ì›ì‹ ", "ì  ë ˆìŠ¤ ì¡´ ì œë¡œ", "ìŠ¤íƒ€ë ˆì¼", "ë¶•ê´´",
    # ì¼ë³¸ì–´
    "å´©å£Šï¼šã‚¹ã‚¿ãƒ¼ãƒ¬ã‚¤ãƒ«", "ãƒ–ãƒ«ãƒ¼ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–", "ã‚¼ãƒ³ãƒ¬ã‚¹ã‚¾ãƒ¼ãƒ³ã‚¼ãƒ­", "ãƒ›ãƒ¨ãƒãƒ¼ã‚¹", "ã‚²ãƒ¼ãƒ ã‚·ãƒ§ã‚¦", "äºŒæ¬¡å‰µä½œ", "ã‚¬ãƒãƒ£", "ç¾å°‘å¥³",
    # ì¤‘êµ­ì–´
    "ç±³å“ˆæ¸¸", "å´©å", "è“æ¡£æ¡ˆ", "åŸç¥", "å°‘å¥³æ”¶é›†", "äºŒæ¬¡å…ƒ", "é›†æ¢å¼", "åˆä½œ", "å‘å”®", "è™šæ‹Ÿä¸»æ’­"
]

keywords = [k.lower() for k in keywords]

# ğŸ” ì‚¬ì´íŠ¸ë³„ HTML êµ¬ì¡°ë³„ ë¶„ê¸° ì²˜ë¦¬
news_items = []

# âœ… í•œêµ­ ë§¤ì²´ (ì¼ë°˜ êµ¬ì¡°)
korean_sites = [
    "https://www.inven.co.kr/webzine/news/",
    "https://www.thisisgame.com/webzine/news/nboard/263/?category=2",
    "https://www.ezyeconomy.com/news/articleList.html?sc_sub_section_code=S2N71&view_type=sm"
]

for url in korean_sites:
    try:
        res = requests.get(url, timeout=5)
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, "html.parser")
            for a in soup.select("a[href]"):
                title = a.get_text(strip=True)
                href = a["href"]
                if title and href.startswith("http") and any(k in title.lower() for k in keywords):
                    news_items.append({"title": title, "url": href})
    except Exception as e:
        print(f"[í•œêµ­ ë§¤ì²´ ì˜¤ë¥˜] {url} - {e}")

# âœ… ì¼ë³¸ ë§¤ì²´ êµ¬ì¡° ì²˜ë¦¬
japan_sites = [
    "https://gamebiz.jp/news",        # Gamebiz
    "https://www.4gamer.net/",        # 4Gamer
    "https://www.gamer.ne.jp/"        # Gamer.ne.jp
]

for url in japan_sites:
    try:
        res = requests.get(url, timeout=5, headers={"User-Agent": "Mozilla/5.0"})
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, "html.parser")
            for a in soup.select("a[href]"):
                title = a.get_text(strip=True)
                href = a["href"]
                full_url = href if href.startswith("http") else requests.compat.urljoin(url, href)
                if title and any(k in title.lower() for k in keywords):
                    news_items.append({"title": title, "url": full_url})
    except Exception as e:
        print(f"[ì¼ë³¸ ë§¤ì²´ ì˜¤ë¥˜] {url} - {e}")

# âœ… ì¤‘êµ­ ë§¤ì²´ êµ¬ì¡° ì²˜ë¦¬
china_sites = [
    "https://www.youxituoluo.com/",
    "http://news.17173.com/quanqiu/",
    "https://www.163.com/dy/media/T1439279320876.html",
    "https://gnn.gamer.com.tw/index.php?k=4",
    "https://news.qq.com/"
]

for url in china_sites:
    try:
        res = requests.get(url, timeout=5, headers={"User-Agent": "Mozilla/5.0"})
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, "html.parser")
            for a in soup.select("a[href]"):
                title = a.get_text(strip=True)
                href = a["href"]
                full_url = href if href.startswith("http") else requests.compat.urljoin(url, href)
                if title and any(k in title.lower() for k in keywords):
                    news_items.append({"title": title, "url": full_url})
    except Exception as e:
        print(f"[ì¤‘êµ­ ë§¤ì²´ ì˜¤ë¥˜] {url} - {e}")

# âœ… ì¤‘ë³µ ì œê±°
unique_items = {item["url"]: item for item in news_items}
news_items = list(unique_items.values())

# âœ… ì €ì¥
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(news_items, f, ensure_ascii=False, indent=2)

print(f"âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_items)}ê±´ â†’ {output_path}")
