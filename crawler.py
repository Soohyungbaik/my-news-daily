import requests
from bs4 import BeautifulSoup
from datetime import datetime
import os
import smtplib
from email.mime.text import MIMEText
import shutil

# ì˜¤ëŠ˜ ë‚ ì§œ
today = datetime.today().strftime('%Y-%m-%d')

# ë‰´ìŠ¤ ì†ŒìŠ¤ URL
source_url = f"https://soohyungbaik.github.io/my-news-daily/dailynews/{today}.html"
res = requests.get(source_url)

# í‚¤ì›Œë“œ ë¶ˆëŸ¬ì˜¤ê¸°
if os.path.exists('keywords.txt'):
    with open('keywords.txt', 'r', encoding='utf-8') as f:
        keywords = [line.strip().lower() for line in f if line.strip()]
else:
    keywords = []

# ë§¤ì²´ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
if os.path.exists('media_list.txt'):
    with open('media_list.txt', 'r', encoding='utf-8') as f:
        media_list = [line.strip().lower() for line in f if line.strip()]
else:
    media_list = []

# HTML ì‹œì‘
html = f"""
<html><head><meta charset='UTF-8'>
<style>
  body {{ font-family: sans-serif; }}
  .item {{ margin-bottom: 10px; }}
</style></head>
<body>
<h2>[ë‰´ìŠ¤ë ˆí„°] {today}</h2>
<ul>
"""

filtered = []
matching_urls = []  # ğŸ” í•„í„° ì¡°ê±´ì— ë§¤ì¹­ëœ ëª¨ë“  ë‰´ìŠ¤ì˜ URL ì €ì¥

if res.status_code == 200:
    soup = BeautifulSoup(res.text, 'html.parser')
    items = soup.select('li > a')

    for item in items:
        title = item.text.strip()
        url = item['href'].strip()
        lower_title = title.lower()
        lower_url = url.lower()

        try:
            article_res = requests.get(url, timeout=3)
            article_text = article_res.text.lower() if article_res.status_code == 200 else ''
        except:
            article_text = ''

        keyword_match = any(k in lower_title or k in article_text for k in keywords)
        media_match = any(m in lower_url for m in media_list)

        if (not keywords and not media_list) or keyword_match or media_match:
            filtered.append((title, url))
            matching_urls.append(url)
        elif keyword_match or media_match:
            matching_urls.append(url)

    if filtered:
        for title, url in filtered:
            html += f"<li class='item'><a href='{url}'>{title}</a></li>"
    else:
        html += "<li class='item'><i>ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.</i></li>"
        if matching_urls:
            html += "<li><strong>ğŸ“Œ í‚¤ì›Œë“œ/ë§¤ì²´ì— ë§¤ì¹­ëœ URL ëª©ë¡:</strong></li>"
            for u in matching_urls:
                html += f"<li><a href='{u}'>{u}</a></li>"
        elif keywords:
            html += "<li><strong>ğŸ“Œ ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ ëª©ë¡:</strong></li>"
            for kw in keywords:
                html += f"<li>- {kw}</li>"
else:
    html += "<li class='item'><i>ê¸ˆì¼ ë‰´ìŠ¤ ì†ŒìŠ¤ê°€ ì—†ì–´ í‚¤ì›Œë“œë§Œ ì œê³µí•©ë‹ˆë‹¤.</i></li>"
    if keywords:
        html += "<li><strong>ğŸ“Œ ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ ëª©ë¡:</strong></li>"
        for kw in keywords:
            html += f"<li>- {kw}</li>"

html += "</ul></body></html>"

# daily_html ë””ë ‰í† ë¦¬ ì²˜ë¦¬
output_dir = "daily_html"
if os.path.exists(output_dir):
    if not os.path.isdir(output_dir):
        print(f"âš ï¸ '{output_dir}'ëŠ” íŒŒì¼ì…ë‹ˆë‹¤. ì‚­ì œ í›„ ë””ë ‰í† ë¦¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        os.remove(output_dir)
        os.makedirs(output_dir)
    else:
        os.makedirs(output_dir, exist_ok=True)
else:
    os.makedirs(output_dir)

# HTML ì €ì¥
output_path = f"{output_dir}/{today}.html"
with open(output_path, 'w', encoding='utf-8') as f:
    f.write(html)

# index.html ê°±ì‹ 
index_path = "index.html"
if os.path.exists(index_path):
    if not os.path.isfile(index_path):
        print(f"âš ï¸ '{index_path}'ëŠ” íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤. ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        shutil.rmtree(index_path)
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write("""<html><head><meta charset="UTF-8"><title>ë‰´ìŠ¤ ëª¨ìŒ</title></head>
  <body><h1>ë‰´ìŠ¤ ëª¨ìŒ</h1><ul>
    <!-- ë‹¤ìŒ ë‚ ì§œê°€ ìƒê¸°ë©´ crawler.pyê°€ ìë™ ì¶”ê°€ -->
  </ul></body></html>""")
    else:
        with open(index_path, 'r', encoding='utf-8') as f:
            content = f.read()
        if "<ul>" not in content:
            with open(index_path, 'w', encoding='utf-8') as f:
                f.write("""<html><head><meta charset="UTF-8"><title>ë‰´ìŠ¤ ëª¨ìŒ</title></head>
  <body><h1>ë‰´ìŠ¤ ëª¨ìŒ</h1><ul>
    <!-- ë‹¤ìŒ ë‚ ì§œê°€ ìƒê¸°ë©´ crawler.pyê°€ ìë™ ì¶”ê°€ -->
  </ul></body></html>""")
else:
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write("""<html><head><meta charset="UTF-8"><title>ë‰´ìŠ¤ ëª¨ìŒ</title></head>
  <body><h1>ë‰´ìŠ¤ ëª¨ìŒ</h1><ul>
    <!-- ë‹¤ìŒ ë‚ ì§œê°€ ìƒê¸°ë©´ crawler.pyê°€ ìë™ ì¶”ê°€ -->
  </ul></body></html>""")

# ë‚ ì§œ ë§í¬ ì‚½ì…
with open(index_path, 'r', encoding='utf-8') as f:
    index_html = f.read()

new_entry = f"<li><a href=\"{output_dir}/{today}.html\">{today}</a></li>"
if new_entry not in index_html:
    index_html = index_html.replace(
        "<!-- ë‹¤ìŒ ë‚ ì§œê°€ ìƒê¸°ë©´ crawler.pyê°€ ìë™ ì¶”ê°€ -->",
        f"{new_entry}\n    <!-- ë‹¤ìŒ ë‚ ì§œê°€ ìƒê¸°ë©´ crawler.pyê°€ ìë™ ì¶”ê°€ -->"
    )
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(index_html)

# ì´ë©”ì¼ ë°œì†¡
msg = MIMEText(html, 'html')
msg['Subject'] = f"[ë‰´ìŠ¤ë ˆí„°] {today}"
msg['From'] = os.getenv("EMAIL_FROM")
msg['To'] = os.getenv("EMAIL_TO")

try:
    server = smtplib.SMTP_SSL('smtp.gmail.com', 465)
    server.login(os.getenv("EMAIL_FROM"), os.getenv("EMAIL_PASSWORD"))
    server.send_message(msg)
    server.quit()
    print("âœ… ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ")
except Exception as e:
    print("âŒ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨:", e)

print(f"âœ… ë‰´ìŠ¤ HTML ìƒì„± ì™„ë£Œ: {output_path}")


