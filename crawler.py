import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from email.mime.text import MIMEText
import smtplib
from bs4 import XMLParsedAsHTMLWarning
import warnings

# XML ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

today = datetime.today().strftime('%Y-%m-%d')
source_url = f"https://soohyungbaik.github.io/my-news-daily/dailynews/{today}.html"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# í‚¤ì›Œë“œ ë° ë§¤ì²´ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
keywords = []
if os.path.exists('keywords.txt'):
    with open('keywords.txt', 'r', encoding='utf-8') as f:
        keywords = [line.strip().lower() for line in f if line.strip()]

media_list = []
if os.path.exists('media_list.txt'):
    with open('media_list.txt', 'r', encoding='utf-8') as f:
        media_list = [line.strip().lower() for line in f if line.strip()]

# HTML ë¶ˆëŸ¬ì˜¤ê¸° (ì›ê²© or ë¡œì»¬)
try:
    res = requests.get(source_url, headers=HEADERS)
    res.raise_for_status()
    res.encoding = res.apparent_encoding
    html_text = res.text
    print(f"âœ… ì›ê²© ë‰´ìŠ¤ íŒŒì¼ ìš”ì²­ ì„±ê³µ: {source_url}")
except Exception:
    local_path = f"dailynews/{today}.html"
    if os.path.exists(local_path):
        print(f"âš ï¸ ì›ê²© ìš”ì²­ ì‹¤íŒ¨, ë¡œì»¬ íŒŒì¼ë¡œ ëŒ€ì²´: {local_path}")
        with open(local_path, 'r', encoding='utf-8') as f:
            html_text = f.read()
    else:
        print("âŒ ì›ê²© ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨ ë° ë¡œì»¬ íŒŒì¼ë„ ì—†ìŒ")
        html_text = None

# ë‰´ìŠ¤ ì œëª© ë³´ì™„ í•¨ìˆ˜
def get_article_title(url):
    try:
        res = requests.get(url, headers=HEADERS, timeout=5)
        if res.status_code == 200:
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, "html.parser")

            if "17173.com" in url:
                h1 = soup.find("h1")
                if h1:
                    return h1.text.strip()
                title_tag = soup.find("title")
                if title_tag:
                    return title_tag.text.strip()

            og_title = soup.find("meta", property="og:title")
            if og_title and og_title.get("content"):
                return og_title["content"].strip()
    except Exception as e:
        print(f"âš ï¸ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
    return None

# HTML í…œí”Œë¦¿ ì‹œì‘
html = f"""<html><head><meta charset='UTF-8'>
<style>
  body {{ font-family: sans-serif; }}
  .item {{ margin-bottom: 10px; }}
</style></head>
<body>
<h2>[ë‰´ìŠ¤ë ˆí„°] {today}</h2>
<ul>
"""

filtered = []
matching_urls = []

if html_text:
    soup = BeautifulSoup(html_text, 'html.parser')
    items = soup.select('li > a')
    checked = 0
    max_links = 30

    for item in items:
        if checked >= max_links:
            break
        url = item.get('href', '').strip()
        title = item.text.strip()
        lower_title = title.lower()
        lower_url = url.lower()
        checked += 1

        try:
            article_res = requests.get(url, headers=HEADERS, timeout=5)
            if article_res.status_code == 200:
                article_res.encoding = article_res.apparent_encoding
                article_text = article_res.text.lower()
                full_title = get_article_title(url) or title
            else:
                article_text = ''
                full_title = title
        except:
            article_text = ''
            full_title = title

        keyword_match = any(k in lower_title or k in article_text for k in keywords)
        media_match = any(m in lower_url for m in media_list)

        if keyword_match or media_match:
            filtered.append((full_title, url))
            matching_urls.append(url)

    if filtered:
        for title, url in filtered:
            html += f"<li class='item'><a href='{url}'>{title}</a></li>"
    else:
        html += "<li class='item'><i>ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.</i></li>"
        if matching_urls:
            html += "<li><strong>ğŸ“Œ í‚¤ì›Œë“œ/ë§¤ì²´ì— ë§¤ì¹­ëœ URL ëª©ë¡:</strong></li>"
            for u in matching_urls:
                html += f"<li><a href='{u}'>{u}</a></li>"
        elif keywords:
            html += "<li><strong>ğŸ“Œ ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ ëª©ë¡:</strong></li>"
            for kw in keywords:
                html += f"<li>- {kw}</li>"
else:
    html += "<li class='item'><i>ê¸ˆì¼ ë‰´ìŠ¤ ì†ŒìŠ¤ê°€ ì—†ì–´ í‚¤ì›Œë“œë§Œ ì œê³µí•©ë‹ˆë‹¤.</i></li>"
    if keywords:
        html += "<li><strong>ğŸ“Œ ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ ëª©ë¡:</strong></li>"
        for kw in keywords:
            html += f"<li>- {kw}</li>"

html += "</ul></body></html>"

# ì €ì¥
output_dir = "daily_html"
os.makedirs(output_dir, exist_ok=True)
output_path = f"{output_dir}/{today}.html"
with open(output_path, 'w', encoding='utf-8') as f:
    f.write(html)
print(f"âœ… ë‰´ìŠ¤ HTML ìƒì„± ì™„ë£Œ: {output_path}")

# index.html ê°±ì‹ 
index_path = "index.html"
if not os.path.exists(index_path):
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write("<html><head><meta charset='UTF-8'><title>ë‰´ìŠ¤ ëª¨ìŒ</title></head><body><h1>ë‰´ìŠ¤ ëª¨ìŒ</h1><ul>\n<!-- ë‹¤ìŒ ë‚ ì§œê°€ ìƒê¸°ë©´ crawler.pyê°€ ìë™ ì¶”ê°€ -->\n</ul></body></html>")

with open(index_path, 'r', encoding='utf-8') as f:
    index_html = f.read()

new_entry = f"<li><a href=\"{output_dir}/{today}.html\">{today}</a></li>"
if new_entry not in index_html:
    index_html = index_html.replace("<!-- ë‹¤ìŒ ë‚ ì§œê°€ ìƒê¸°ë©´ crawler.pyê°€ ìë™ ì¶”ê°€ -->", f"{new_entry}\n<!-- ë‹¤ìŒ ë‚ ì§œê°€ ìƒê¸°ë©´ crawler.pyê°€ ìë™ ì¶”ê°€ -->")
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(index_html)

# ì´ë©”ì¼ ë°œì†¡
msg = MIMEText(html, 'html')
msg['Subject'] = f"[ë‰´ìŠ¤ë ˆí„°] {today}"
msg['From'] = os.getenv("EMAIL_FROM")
msg['To'] = os.getenv("EMAIL_TO")

try:
    server = smtplib.SMTP_SSL('smtp.gmail.com', 465)
    server.login(os.getenv("EMAIL_FROM"), os.getenv("EMAIL_PASSWORD"))
    server.send_message(msg)
    server.quit()
    print("âœ… ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ")
except Exception as e:
    print("âŒ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨:", e)


