import datetime
import os
import re
import requests
from bs4 import BeautifulSoup

today = datetime.date.today().strftime('%Y-%m-%d')
output_dir = "dailynews"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, f"{today}.html")

# ğŸ” í‚¤ì›Œë“œ
keywords = [
    # í•œêµ­ì–´
    "ì„œë¸Œì»¬ì²˜", "ìˆ˜ì§‘í˜•", "ë¯¸ì†Œë…€", "ê²Œì„ì‡¼", "êµ¿ìŠ¤ë§ˆì¼", "ì½”ìŠ¤í”„ë ˆ", "ë¶€ìŠ¤", "ì½œë¼ë³´", "ëŸ°ì¹­", "ì—…ê³„ ë™í–¥", "ì‹œì¥ ë³´ê³ ì„œ",
    "ë‹ˆì¼€", "ë¸”ë£¨ì•„ì¹´ì´ë¸Œ", "ì›ì‹ ", "ì  ë ˆìŠ¤ ì¡´ ì œë¡œ", "ìŠ¤íƒ€ë ˆì¼", "ë¶•ê´´",
    # ì¼ë³¸ì–´
    "å´©å£Šï¼šã‚¹ã‚¿ãƒ¼ãƒ¬ã‚¤ãƒ«", "ãƒ–ãƒ«ãƒ¼ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–", "ã‚¼ãƒ³ãƒ¬ã‚¹ã‚¾ãƒ¼ãƒ³ã‚¼ãƒ­", "ãƒ›ãƒ¨ãƒãƒ¼ã‚¹", "ã‚²ãƒ¼ãƒ ã‚·ãƒ§ã‚¦", "äºŒæ¬¡å‰µä½œ", "ã‚¬ãƒãƒ£", "ç¾å°‘å¥³",
    # ì¤‘êµ­ì–´
    "ç±³å“ˆæ¸¸", "å´©å", "è“æ¡£æ¡ˆ", "åŸç¥", "å°‘å¥³æ”¶é›†", "äºŒæ¬¡å…ƒ", "é›†æ¢å¼", "åˆä½œ", "å‘å”®", "è™šæ‹Ÿä¸»æ’­",
    # ì˜ì–´
    "Zenless Zone Zero", "Blue Archive", "Nikke"
]

news_items = []

# ğŸ”§ ì œëª© ì •ì œ
def clean_title(raw):
    clean = re.split(r'[|ï½œ\-â€“â€”:\[\]]', raw)[0].strip()
    return clean[:20] + "..." if len(clean) > 20 else clean

# ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘ ê³µí†µ í•¨ìˆ˜
def collect_news_from(sites, region, selector="a[href]", force_encoding=None):
    print(f"ğŸŸ¡ [{region}] ìˆ˜ì§‘ ì‹œì‘")
    for url in sites:
        try:
            res = requests.get(url, timeout=5)
            if force_encoding:
                res.encoding = force_encoding  # ì¸ì½”ë”© ê°•ì œ ì„¤ì •
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, "html.parser")
                links = soup.select(selector)
                match_count = 0
                for link in links:
                    raw_title = link.get_text(strip=True)
                    title = clean_title(raw_title)
                    href = link.get("href", "")
                    if not href.startswith("http"):
                        continue
                    if title and any(k.lower() in title.lower() for k in keywords):
                        news_items.append({"title": title, "url": href})
                        match_count += 1
                    else:
                        print(f"[{region}] ë¯¸ë§¤ì¹­: {title}")
                print(f"âœ… [{region}] {url} - ë§¤ì¹­ {match_count}ê±´")
            else:
                print(f"âŒ [{region}] {url} - ì‘ë‹µ ì½”ë“œ {res.status_code}")
        except Exception as e:
            print(f"âŒ [{region}] {url} - ì˜ˆì™¸ ë°œìƒ: {e}")

# âœ… ì‚¬ì´íŠ¸ ëª©ë¡
korea_sites = [
    "https://www.inven.co.kr/webzine/news/",
    "https://www.thisisgame.com/webzine/news/nboard/263/?category=2",
    "https://www.ezyeconomy.com/news/articleList.html?sc_sub_section_code=S2N71&view_type=sm"
]

japan_sites = [
    "https://gamebiz.jp/news",
    "https://www.4gamer.net/",
    "https://www.gamer.ne.jp/",
    "https://gnn.gamer.com.tw/index.php?k=4"
]

china_sites = [
    "https://www.17173.com/",
    "https://www.youxituoluo.com/",
    "https://www.163.com/dy/media/T1439279320876.html",
    "https://news.qq.com/"
]

# âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰
collect_news_from(korea_sites, "í•œêµ­")
collect_news_from(japan_sites, "ì¼ë³¸", force_encoding="utf-8")  # ì¼ë³¸ì€ UTF-8 ê°•ì œ
collect_news_from(china_sites, "ì¤‘êµ­", force_encoding="utf-8")  # ì¤‘êµ­ë„ UTF-8 ê¸°ë³¸ ì‹œë„

# âœ… HTML ìƒì„±
html = f"""<html><head><meta charset='UTF-8'>
<style>
  body {{ font-family: sans-serif; }}
  .item {{ margin-bottom: 10px; }}
</style></head>
<body>
<h2>[ë‰´ìŠ¤ë ˆí„°] {today}</h2>
<ul>
"""

if not news_items:
    html += "<li class='item'><i>ê¸ˆì¼ ë‰´ìŠ¤ ì†ŒìŠ¤ê°€ ì—†ì–´ í‚¤ì›Œë“œë§Œ ì œê³µë©ë‹ˆë‹¤.</i></li>"
    for kw in keywords:
        html += f"<li>- {kw}</li>"
else:
    for item in news_items:
        html += f"<li class='item'><a href='{item['url']}'>{item['title']}</a></li>"

html += "</ul></body></html>"

with open(output_path, "w", encoding="utf-8") as f:
    f.write(html)

print(f"âœ… ë‰´ìŠ¤ HTML ìƒì„± ì™„ë£Œ: {output_path}")
